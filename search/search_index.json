{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Guard - The Security Toolkit for LLM Interactions","text":"<p>LLM Guard by Laiyer.ai is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).</p> <p>Playground | Changelog | Blog | Slack</p>"},{"location":"#what-is-llm-guard","title":"What is LLM Guard?","text":"<p>By offering sanitization, detection of harmful language, prevention of data leakage, and resistance against prompt injection attacks, LLM-Guard ensures that your interactions with LLMs remain safe and secure.</p>"},{"location":"#installation","title":"Installation","text":"<p>Begin your journey with LLM Guard by downloading the package:</p> <pre><code>pip install llm-guard\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Important Notes:</p> <ul> <li>LLM Guard is designed for easy integration and deployment in production environments. While it's ready to use   out-of-the-box, please be informed that we're constantly improving and updating the repository.</li> <li>Base functionality requires a limited number of libraries. As you explore more advanced features, necessary libraries   will be automatically installed.</li> <li>Ensure you're using Python version 3.9 or higher. Confirm with: <code>python --version</code>.</li> <li>Library installation issues? Consider upgrading pip: <code>python -m pip install --upgrade pip</code>.</li> </ul> <p>Examples:</p> <ul> <li>Get started with ChatGPT and LLM Guard.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#general","title":"General","text":"<ul> <li> Extend language support to cover popular and emerging languages, prioritize based on community feedback.</li> <li> Allow comparison of multiple outputs to facilitate better analysis and choice.</li> <li> Enable scanning of logits to support streaming mode.</li> <li> Expand examples and integrations, ensuring they cover common use-cases and are easy to follow.</li> </ul>"},{"location":"#prompt-scanners","title":"Prompt Scanners","text":"<ul> <li> Utilize expressions for code detection to reduce dependency on models, improving speed and reliability.</li> <li> Support a variety of token calculators to offer more flexibility and compatibility.</li> </ul>"},{"location":"#output-scanners","title":"Output Scanners","text":"<ul> <li> Scan for vulnerable libraries and provide recommendations for safer alternatives.</li> <li> Check for license compliance to ensure legal integrity.</li> <li> Detect insecure code patterns.</li> <li> Identify potential SQL injection points to enhance security.</li> <li> Verify links and provide options for whitelisting or blacklisting to maintain the quality of references.</li> </ul>"},{"location":"#community-contributing-docs-support","title":"Community, Contributing, Docs &amp; Support","text":"<p>LLM Guard is an open source solution. We are committed to a transparent development process and highly appreciate any contributions. Whether you are helping us fix bugs, propose new features, improve our documentation or spread the word, we would love to have you as part of our community.</p> <ul> <li>Give us a \u2b50\ufe0f github star \u2b50\ufe0f on the top of this page to support what we're doing,   it means a lot for open source projects!</li> <li>Read our   docs   for more info about how to use and customize deepchecks, and for step-by-step tutorials.</li> <li>Post a Github   Issue to submit a bug report, feature request, or suggest an improvement.</li> <li>To contribute to the package, check out our contribution guidelines, and open a PR.</li> </ul> <p>Join our Slack to give us feedback, connect with the maintainers and fellow users, ask questions, get help for package usage or contributions, or engage in discussions about LLM security!</p> <p></p>"},{"location":"#supporters","title":"Supporters","text":"<p>LLM Guard is supported by the following organizations:</p> <ul> <li>Google Patch Rewards program</li> <li>JetBrains</li> </ul>"},{"location":"best_practices/","title":"Best Practices","text":""},{"location":"best_practices/#performance-optimization","title":"Performance Optimization","text":"<ol> <li> <p>Benchmark Analysis: Before choosing the scanners, it's crucial to understand their performance on different instances. Review the benchmarks for each scanner to make an informed decision based on your specific requirements.</p> </li> <li> <p>Model Size Trade-off: Opting for smaller models will expedite processing, reducing latency. However, this comes at the cost of accuracy. We are actively working on providing compact versions with minimal accuracy trade-offs.</p> </li> <li> <p>Use ONNX Runtime for CPU: ONNX Runtime is a high-performance inference engine for machine learning models. When possible, we recommend using ONNX Runtime for serving the models.</p> </li> </ol>"},{"location":"best_practices/#serving-configurations","title":"Serving Configurations","text":"<ol> <li> <p>Fast Failure Mode: Enable the <code>fail_fast</code> mode while serving to ensure early exits, preventing the wait for all scanners to complete, thus optimizing the response time.</p> </li> <li> <p>Scanner Selection: Assess the relevance of different scanners for your use-case. Instead of employing all scanners synchronously, which might overwhelm the system, consider using them asynchronously. This approach enhances observability, aiding in precise debugging and performance monitoring.</p> </li> <li> <p>Request Sampling: Run slower scanners on a sample of requests to reduce the overall latency. This approach is especially useful when the system is under heavy load.</p> </li> </ol>"},{"location":"best_practices/#observability-and-debugging","title":"Observability and Debugging","text":"<ol> <li>Logging and Metrics: Implement robust logging and metric collection to monitor the system's performance and health.</li> </ol>"},{"location":"best_practices/#continuous-improvement","title":"Continuous Improvement","text":"<ol> <li> <p>Feedback Loops: Establish feedback loops with your system's users to understand how the library is performing in real-world scenarios, and to gather suggestions for improvements.</p> </li> <li> <p>Regular Updates and Testing: Stay updated with the latest versions of <code>llm-guard</code>, and ensure thorough testing in a staging environment before rolling out updates in a production setup.</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased-034","title":"Unreleased - 0.3.4","text":""},{"location":"changelog/#added","title":"Added","text":"<p>-</p>"},{"location":"changelog/#fixed","title":"Fixed","text":"<p>-</p>"},{"location":"changelog/#changed","title":"Changed","text":"<p>-</p>"},{"location":"changelog/#removed","title":"Removed","text":"<p>-</p>"},{"location":"changelog/#033-2023-11-25","title":"0.3.3 - 2023-11-25","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Benchmarks on Azure instances</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Upgraded <code>json_repair</code> library (issue)</li> <li>Use proprietary prompt injection detection model laiyer/deberta-v3-base-prompt-injection</li> </ul>"},{"location":"changelog/#032-2023-11-15","title":"0.3.2 - 2023-11-15","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Using ONNX converted models hosted by Laiyer on HuggingFace</li> <li>Switched to better model for MaliciousURLs scanner - DunnBC22/codebert-base-Malicious_URLs</li> <li><code>BanTopics</code>, <code>NoRefusal</code>, <code>FactualConsistency</code> and <code>Relevance</code> scanners support ONNX inference</li> <li><code>Relevance</code> rely on optimized ONNX models</li> <li>Switched to using <code>transformers</code> in <code>Relevance</code> scanner to have less dependencies</li> <li>Updated benchmarks for relevant scanners</li> <li>Use <code>papluca/xlm-roberta-base-language-detection</code> model for the <code>Language</code> and <code>LanguageSame</code> scanner</li> <li><code>PromptInjection</code> calculates risk score based on the defined threshold</li> <li>Up-to-date Langchain integration using LCEL</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Remove <code>lingua-language-detector</code> dependency from <code>Language</code> and <code>LanguageSame</code> scanners</li> </ul>"},{"location":"changelog/#031-2023-11-09","title":"0.3.1 - 2023-11-09","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Handling long prompts by truncating it to the maximum length of the model</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Use single <code>PromptInjection</code> scanner with multiple models</li> <li>Benchmarks are measured for each scanner individually</li> <li>In the <code>Refutation</code> output scanner use the same model for the NLI as used in the <code>BanTopics</code></li> <li>Benchmarks for each individual scanner instead of one common</li> <li>Use <code>deepset/deberta-v3-base-injection</code> model for the <code>PromptInjection</code> scanner</li> <li>Optimization of scanners on GPU by using <code>batch_size=1</code></li> <li>Use <code>lingua-language-detector</code> instead of <code>langdetect</code> in the <code>Language</code> scanner</li> <li>Upgrade all libraries including <code>transformers</code> to the latest versions</li> <li>Use Transformers recognizers in the <code>Anonymize</code> and <code>Sensitive</code> scanner to improve named-entity recognition</li> <li>Possibility of using ONNX runtime in scanners by enabling <code>use_onnx</code> parameter</li> <li>Use the newest <code>MoritzLaurer/deberta-v3-base-zeroshot-v1</code> model for the <code>BanTopics</code> and <code>Refutation</code> scanners</li> <li>Use the newest <code>MoritzLaurer/deberta-v3-large-zeroshot-v1</code> model for the <code>NoRefusal</code> scanner</li> <li>Use better <code>unitary/unbiased-toxic-roberta</code> model for Toxicity scanners (both input and output)</li> <li>ONNX on API deployment for faster CPU inference</li> <li>CUDA on API deployment for faster GPU inference</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Remove <code>PromptInjectionV2</code> scanner to rely on the single one with a choice</li> <li>Langchain <code>LLMChain</code> example as this functionality is deprecated, use <code>LCEL</code> instead</li> </ul>"},{"location":"changelog/#030-2023-10-14","title":"0.3.0 - 2023-10-14","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li><code>Regex</code> scanner to the prompt</li> <li><code>Language</code> scanners both for prompt and output</li> <li><code>JSON</code> output scanner</li> <li>Best practices to the documentation</li> <li><code>LanguageSame</code> output scanner to check that the prompt and output languages are the same</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li><code>BanSubstrings</code> can match all substrings in addition to any of them</li> <li><code>Sensitive</code> output scanner can redact found entities</li> <li>Change to faster model for <code>BanTopics</code> prompt and output scanners MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c</li> <li>Changed model for the <code>NoRefusal</code> scanner to faster MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c</li> <li><code>Anonymize</code> and <code>Sensitive</code> scanners support more accurate models (e.g. beki/en_spacy_pii_distilbert and ability to choose them. It also reduced the latency of this scanner</li> <li>Usage of <code>sentence-transformers</code> library replaced with <code>FlagEmbedding</code> in the <code>Relevance</code> output scanner</li> <li>Ability to choose embedding model in <code>Relevance</code> scanner and use the best model currently available</li> <li>Cache tokenizers in memory to improve performance</li> <li>Moved API deployment to <code>llm_guard_api</code></li> <li><code>JSON</code> scanner can repair the JSON if it is broken</li> <li>Rename <code>Refutation</code> scanner to <code>FactualConsistency</code> to better reflect its purpose</li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Removed chunking in <code>Anonymize</code> and <code>Sensitive</code> scanners because it was breaking redaction</li> </ul>"},{"location":"changelog/#024-2023-10-07","title":"0.2.4 - 2023-10-07","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Langchain example using LangChain Expression Language (LCEL)</li> <li>Added prompt injection scanner v2 model based on hubert233/GPTFuzz</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Using another Bias detection model which works better on different devices valurank/distilroberta-bias</li> <li>Updated the roadmap in README and documentation</li> <li><code>BanSubstrings</code> can redact found substrings</li> <li>One <code>logger</code> for all scanners</li> <li><code>device</code> became function to lazy load (avoid <code>torch</code> import when unnecessary)</li> <li>Lazy load dependencies in scanners</li> <li>Added elapsed time in logs of <code>evaluate_prompt</code> and <code>evaluate_output</code> functions</li> <li>New secrets detectors</li> <li>Added GPU benchmarks on <code>g5.xlarge</code> instance</li> <li>Tests are running on Python 3.9, 3.10 and 3.11</li> </ul>"},{"location":"changelog/#removed_4","title":"Removed","text":"<ul> <li>Usage of <code>accelerate</code> library for inference. Instead, it will detect device using <code>torch</code></li> </ul>"},{"location":"changelog/#023-2023-09-23","title":"0.2.3 - 2023-09-23","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Added Swagger documentation on the API documentation page</li> <li>Added <code>fail_fast</code> flag to stop the execution after the first failure</li> <li>Updated API and Playground to support <code>fail_fast</code> flag</li> <li>Clarified order of execution in the documentation</li> <li>Added timeout configuration for API example</li> <li>Better examples of <code>langchain</code> integration</li> </ul>"},{"location":"changelog/#022-2023-09-21","title":"0.2.2 - 2023-09-21","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Missing secrets detection for Github token in the final build</li> </ul>"},{"location":"changelog/#021-2023-09-21","title":"0.2.1 - 2023-09-21","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>New pages in the docs about usage of LLM Guard</li> <li>Benchmark of AWS EC2 <code>inf1.xlarge</code> instance</li> <li>Example of API with Docker in llm_guard_api</li> <li><code>Regex</code> output scanner can redact the text using a regular expression</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Lowercase prompt in Relevance output scanner to improve quality of cosine similarity</li> <li>Detect code snippets from Markdown in <code>Code</code> scanner to prevent false-positives</li> <li>Changed model used for <code>PromptInjection</code> to <code>JasperLS/deberta-v3-base-injection</code>, which produces less false-positives</li> <li>Introduced <code>threshold</code> parameter for <code>Code</code> scanners to control the threshold for the similarity</li> </ul>"},{"location":"changelog/#020-2023-09-15","title":"0.2.0 - 2023-09-15","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Documentation moved to <code>mkdocs</code></li> <li>Benchmarks in the documentation</li> <li>Added documentation about adding more scanners</li> <li><code>Makefile</code> with useful commands</li> <li>Demo application using Streamlit deployed to HuggingFace Spaces</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li><code>MaliciousURLs</code> scanner produced false positives when URLs are not extracted from the text</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Support of GPU inference</li> <li>Score of existing <code>Anonymize</code> patterns</li> </ul>"},{"location":"changelog/#removed_5","title":"Removed","text":"<ul> <li><code>URL</code> entity type from <code>Anonymize</code> scanner (it was producing false-positive results)</li> </ul>"},{"location":"changelog/#013-2023-09-02","title":"0.1.3 - 2023-09-02","text":""},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Lock <code>transformers</code> version to 4.32.0 because <code>spacy-transformers</code> require it</li> <li>Update the roadmap based on the feedback from the community</li> <li>Updated <code>NoRefusal</code> scanner to use transformer to classify the output</li> </ul>"},{"location":"changelog/#removed_6","title":"Removed","text":"<ul> <li>Jailbreak input scanner (it was doing the same as the prompt injection one)</li> </ul>"},{"location":"changelog/#012-2023-08-26","title":"0.1.2 - 2023-08-26","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Bias output scanner</li> <li>Sentiment output scanner</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Introduced new linters for markdown</li> </ul>"},{"location":"changelog/#011-2023-08-20","title":"0.1.1 - 2023-08-20","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Example integration with LangChain</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Flow picture instead of the logo</li> <li>Bump libraries</li> </ul>"},{"location":"changelog/#010-2023-08-12","title":"0.1.0 - 2023-08-12","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Refutation output scanner</li> <li>MaliciousURLs output scanner</li> <li>Secrets prompt scanner</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>All prompt scanners: Introducing a risk score, where 0 - means no risk, 1 - means high risk</li> <li>All output scanners: Introducing a risk score, where 0 - means no risk, 1 - means high risk</li> <li>Anonymize prompt scanner: Using the transformer based Spacy model <code>en_core_web_trf</code> (reference)</li> <li>Anonymize prompt scanner: Supporting faker for applicable entities instead of placeholder (<code>use_faker</code> parameter)</li> <li>Anonymize prompt scanner: Remove all patterns for secrets detection, use Secrets prompt scanner instead.</li> <li>Jailbreak prompt scanner: Updated dataset with more examples, removed duplicates</li> </ul>"},{"location":"changelog/#removed_7","title":"Removed","text":"<ul> <li>Anonymize prompt scanner: Removed <code>FILE_EXTENSION</code> entity type</li> </ul>"},{"location":"changelog/#003-2023-08-10","title":"0.0.3 - 2023-08-10","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Dependabot support</li> <li>CodeQL support</li> <li>More pre-commit hooks to improve linters</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Locked libraries in <code>requirements.txt</code></li> <li>Logo link in README</li> </ul>"},{"location":"changelog/#002-2023-08-07","title":"0.0.2 - 2023-08-07","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed missing <code>.json</code> files in the package</li> </ul>"},{"location":"changelog/#001-2023-08-07","title":"0.0.1 - 2023-08-07","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Project structure</li> <li>Documentation</li> <li>Github Actions pipeline</li> <li>Prompt scanners with tests:</li> <li>Anonymize</li> <li>BanSubstrings</li> <li>BanTopics</li> <li>Code</li> <li>PromptInjection</li> <li>Sentiment</li> <li>TokenLimit</li> <li>Toxicity</li> <li>Output scanners with tests:</li> <li>BanSubstrings</li> <li>BanTopics</li> <li>Code</li> <li>Deanonymize</li> <li>NoRefusal</li> <li>Regex</li> <li>Relevance</li> <li>Sensitive</li> <li>Toxicity</li> </ul>"},{"location":"installation/","title":"Installing LLM Guard","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Supported Python versions:</p> <ul> <li>3.9</li> <li>3.10</li> <li>3.11</li> </ul>"},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>Note</p> <p>Consider installing the LLM Guard python packages on a virtual environment like <code>venv</code> or <code>conda</code>.</p> <pre><code>pip install llm-guard\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>To install LLM Guard from source, first clone the repo:</p> <ul> <li>Using HTTPS <pre><code>git clone https://github.com/laiyer-ai/llm-guard.git\n</code></pre></li> <li>Using SSH <pre><code>git clone git@github.com:laiyer-ai/llm-guard.git\n</code></pre></li> </ul> <p>Then, install the package using <code>pip</code>:</p> <pre><code># install the repo\npip install -U -r requirements.txt -r requirements-dev.txt\npython setup.py install\n</code></pre>"},{"location":"quickstart/","title":"Getting started with LLM Guard","text":"<p>Each scanner can be used individually, or using the <code>scan_prompt</code> function.</p>"},{"location":"quickstart/#individual","title":"Individual","text":"<p>You can import an individual scanner and use it to evaluate the prompt or the output:</p> <pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"quickstart/#multiple","title":"Multiple","text":"<p>Info</p> <p>Scanners are executed in the order they are passed to the <code>scan_prompt</code> function.</p> <p>For prompt:</p> <pre><code>from llm_guard import scan_prompt\nfrom llm_guard.input_scanners import Anonymize, PromptInjection, TokenLimit, Toxicity\nfrom llm_guard.vault import Vault\n\nvault = Vault()\ninput_scanners = [Anonymize(vault), Toxicity(), TokenLimit(), PromptInjection()]\n\nsanitized_prompt, results_valid, results_score = scan_prompt(input_scanners, prompt)\nif any(not result for result in results_valid.values()):\n    print(f\"Prompt {prompt} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Prompt: {sanitized_prompt}\")\n</code></pre> <p>For output:</p> <pre><code>from llm_guard import scan_output\nfrom llm_guard.output_scanners import Deanonymize, NoRefusal, Relevance, Sensitive\n\nvault = Vault()\noutput_scanners = [Deanonymize(vault), NoRefusal(), Relevance(), Sensitive()]\n\nsanitized_response_text, results_valid, results_score = scan_output(\n    output_scanners, sanitized_prompt, response_text\n)\nif any(not result for result in results_valid.values()):\n    print(f\"Output {response_text} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Output: {sanitized_response_text}\\n\")\n</code></pre> <p>Note</p> <p>You can set <code>fail_fast</code> to <code>True</code> to stop scanning after the first invalid result. This can help to reduce the latency of the scanning.</p>"},{"location":"customization/add_scanner/","title":"Adding a new scanner","text":"<p>LLM Guard can be extended to support new scanners, and to support additional models for the existing. These scanners could be added via code or ad-hoc as part of the request.</p> <p>Note</p> <p>Before writing code, please read the contributing guide.</p>"},{"location":"customization/add_scanner/#extending-the-input-prompt-scanners","title":"Extending the input (prompt) scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/input_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/input_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/input_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/input_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>docs/changelog.md</code> file.</li> </ol>"},{"location":"customization/add_scanner/#extending-the-output-scanners","title":"Extending the output scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/output_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/output_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/output_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/output_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>docs/changelog.md</code> file.</li> </ol> <p>Info</p> <p>You can use existing scanners as a reference.</p>"},{"location":"input_scanners/anonymize/","title":"Anonymize Scanner","text":"<p>The <code>Anonymize</code> Scanner acts as your digital guardian, ensuring your user prompts remain confidential and free from sensitive data exposure.</p>"},{"location":"input_scanners/anonymize/#what-is-pii","title":"What is PII?","text":"<p>PII, an acronym for Personally Identifiable Information, is the cornerstone of an individual's digital identity. Leaks or mishandling of PII can unleash a storm of problems, from privacy breaches to identity theft. Global regulations, including GDPR and HIPAA, underscore the significance of PII by laying out strict measures for its protection. Furthermore, any unintentional dispatch of PII to LLMs can proliferate this data across various storage points, thus raising the stakes.</p>"},{"location":"input_scanners/anonymize/#attack","title":"Attack","text":"<p>Sometimes, Language Learning Models (or LLMs) can accidentally share private info from the prompts they get. This can be bad because it might let others see or use this info in the wrong way.</p> <p>To stop this from happening, we use the <code>Anonymize</code> scanner. It makes sure user prompts don\u2019t have any private details before the model sees them.</p>"},{"location":"input_scanners/anonymize/#pii-entities","title":"PII Entities","text":"<ul> <li>Credit Cards: Formats mentioned in Wikipedia.</li> <li><code>4111111111111111</code></li> <li><code>378282246310005</code> (American Express)</li> <li><code>30569309025904</code> (Diners Club)</li> <li>Person: A full person name, which can include first names, middle names or initials, and last names.</li> <li><code>John Doe</code></li> <li>PHONE_NUMBER:</li> <li><code>5555551234</code></li> <li>URL: A URL (Uniform Resource Locator), unique identifier used to locate a resource on the Internet.</li> <li><code>https://laiyer.ai</code></li> <li>E-mail Addresses: Standard email formats.</li> <li><code>john.doe@laiyer.ai</code></li> <li><code>john.doe[AT]laiyer[DOT]ai</code></li> <li><code>john.doe[AT]laiyer.ai</code></li> <li><code>john.doe@laiyer[DOT]ai</code></li> <li>IPs: An Internet Protocol (IP) address (either IPv4 or IPv6).</li> <li><code>192.168.1.1</code> (IPv4)</li> <li><code>2001:db8:3333:4444:5555:6666:7777:8888</code> (IPv6)</li> <li>UUID:</li> <li><code>550e8400-e29b-41d4-a716-446655440000</code></li> <li>US Social Security Number (SSN):</li> <li><code>111-22-3333</code></li> <li>Crypto wallet number: Currently only Bitcoin address is supported.</li> <li><code>1Lbcfr7sAHTD9CgdQo3HTMTkV8LK4ZnX71</code></li> <li>IBAN Code: The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank   accounts across national borders to facilitate the communication and processing of cross border transactions with a   reduced risk of transcription errors.</li> <li><code>DE89370400440532013000</code></li> </ul>"},{"location":"input_scanners/anonymize/#features","title":"Features","text":"<ul> <li>Integration with Presidio Analyzer: Leverages the Presidio Analyzer   library, crafted with spaCy, flair and transformers libraries, for precise detection of private data.</li> <li>Enhanced Detection: Beyond Presidio Analyzer's capabilities, the scanner recognizes specific patterns like Email,   US SSN, UUID, and more.</li> <li>Entities Support:</li> <li>Peek at     our default entities.</li> <li>View     the Presidio's supported entities.</li> <li>And, we've     got custom regex patterns     too!</li> <li>Tailored Recognizers:</li> <li>Balance speed vs. accuracy of the recognizers.</li> <li>Top Pick: dslim/bert-base-NER</li> <li>Alternatives: dslim/bert-large-NER.</li> </ul> <p>Info</p> <p>Current entity detection functionality is English-specific.</p>"},{"location":"input_scanners/anonymize/#get-started","title":"Get Started","text":"<p>Initialize the <code>Vault</code>: The Vault archives data that's been redacted.</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Configure the <code>Anonymize</code> Scanner:</p> <pre><code>from llm_guard.input_scanners import Anonymize\nfrom llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF\n\nscanner = Anonymize(vault, preamble=\"Insert before prompt\", allowed_names=[\"John Doe\"], hidden_names=[\"Test LLC\"],\n                    recognizer_conf=BERT_LARGE_NER_CONF)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <ul> <li><code>preamble</code>: Directs the LLM to bypass specific content.</li> <li><code>hidden_names</code>: Transforms specified names to formats like <code>[REDACTED_CUSTOM_1]</code>.</li> <li><code>entity_types</code>: Opt for particular information types to redact.</li> <li><code>regex_pattern_groups_path</code>: Input a path for personalized patterns.</li> <li><code>use_faker</code>: Substitutes eligible entities with fabricated data.</li> <li><code>recognizer_conf</code>: Configures recognizer for the PII data detection.</li> <li><code>threshold</code>: Sets the acceptance threshold (Default: <code>0</code>).</li> </ul> <p>Retrieving Original Data: To revert to the initial data, utilize the Deanonymize scanner.</p>"},{"location":"input_scanners/anonymize/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/anonymize/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"input_scanners/anonymize/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Anonymize\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 317 5 6.11 255.64 294.57 325.71 177.13 1789.64 AWS m5.xlarge with ONNX 317 5 0.73 155.64 169.13 179.93 128.64 2464.29 AWS g5.xlarge GPU 317 5 38.50 321.59 419.60 498.01 125.18 2532.35 Azure Standard_D4as_v4 317 5 48.72 487.29 597.19 685.10 265.64 1193.33 Azure Standard_D4as_v4 with ONNX 317 5 1.47 268.17 286.89 301.87 228.86 1385.13"},{"location":"input_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>Ensure that specific undesired substrings never make it into your prompts with the BanSubstrings scanner.</p>"},{"location":"input_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It is purpose-built to screen user prompts, ensuring none of the banned substrings are present. Users have the flexibility to enforce this check at two distinct granularity levels:</p> <ul> <li> <p>String Level: The banned substring is sought throughout the entire user prompt.</p> </li> <li> <p>Word Level: The scanner exclusively hunts for whole words that match the banned substrings, ensuring no individual   standalone words from the blacklist appear in the prompt.</p> </li> </ul> <p>Additionally, the scanner can be configured to replace the banned substrings with <code>[REDACT]</code> in the model's output.</p>"},{"location":"input_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False, redact=False,\n                        contains_all=False)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>prompt</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: prompt_stop_substrings.json</p>"},{"location":"input_scanners/ban_substrings/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input BanSubstrings\n</code></pre> <p>!!! info:</p> <pre><code>This scanner uses built-in functions, which makes it fast.\n</code></pre>"},{"location":"input_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is a proactive tool aimed at restricting specific topics, such as religion, from being introduced in the prompts. This ensures that interactions remain within acceptable boundaries and avoids potentially sensitive or controversial discussions.</p>"},{"location":"input_scanners/ban_topics/#attack","title":"Attack","text":"<p>Certain topics, when used as prompts for Language Learning Models, can lead to outputs that might be deemed sensitive, controversial, or inappropriate. By banning these topics, service providers can maintain the quality of interactions and reduce the risk of generating responses that could lead to misunderstandings or misinterpretations.</p>"},{"location":"input_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the following models:</p> <ul> <li>MoritzLaurer/deberta-v3-base-zeroshot-v1</li> <li>MoritzLaurer/deberta-v3-large-zeroshot-v1</li> </ul> <p>These models aid in identifying the underlying theme or topic of a prompt, allowing the scanner to cross-check it against a list of banned topics.</p>"},{"location":"input_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/ban_topics/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/ban_topics/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"input_scanners/ban_topics/#use-smaller-models","title":"Use smaller models","text":"<p>You can rely on base model variant (default) to reduce the latency and memory footprint.</p>"},{"location":"input_scanners/ban_topics/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input BanTopics\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 100 5 2.99 471.60 498.70 520.39 416.47 240.11 AWS m5.xlarge with ONNX 100 5 0.11 135.12 139.92 143.77 123.71 808.31 AWS g5.xlarge GPU 100 5 30.46 309.26 396.40 466.11 134.50 743.47 Azure Standard_D4as_v4 100 5 4.00 518.30 547.49 570.85 450.78 221.84 Azure Standard_D4as_v4 with ONNX 100 5 0.02 135.58 136.72 137.63 131.06 763.04"},{"location":"input_scanners/code/","title":"Code Scanner","text":"<p>It is specifically engineered to inspect user prompts and discern if they contain code snippets. It can be particularly useful in platforms that wish to control or monitor the types of programming-related content being queried or in ensuring the appropriate handling of such prompts.</p>"},{"location":"input_scanners/code/#attack","title":"Attack","text":"<p>There are scenarios where the insertion of code in user prompts might be deemed undesirable. Users might be trying to exploit vulnerabilities, test out scripts, or engage in other activities that are outside the platform's intended scope. Monitoring and controlling the nature of the code can be crucial to maintain the integrity and safety of the system.</p>"},{"location":"input_scanners/code/#how-it-works","title":"How it works","text":"<p>Utilizing the prowess of the huggingface/CodeBERTa-language-id model, the scanner can adeptly identify code snippets within prompts across various programming languages. Developers can configure the scanner to either whitelist or blacklist specific languages, thus retaining full control over which types of code can appear in user queries.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <pre><code>- Go\n- Java\n- JavaScript\n- PHP\n- Python\n- Ruby\n</code></pre>"},{"location":"input_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Code\n\nscanner = Code(denied=[\"python\"])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/code/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/code/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by using the ONNX converted model laiyer/CodeBERTa-language-id-onnx.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"input_scanners/code/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Code\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 248 5 2.84 139.08 165.71 187.02 85.69 2894.22 AWS m5.xlarge with ONNX 248 5 0.00 56.40 56.90 57.29 55.32 4481.42 AWS g5.xlarge 248 5 32.56 280.27 370.38 442.47 99.63 2489.33 Azure Standard_D4as_v4 248 5 3.61 156.96 186.50 210.14 95.88 2586.50 Azure Standard_D4as_v4 with ONNX 248 5 0.00 39.36 39.87 40.27 38.00 6525.72"},{"location":"input_scanners/language/","title":"Language Scanner","text":"<p>This scanner identifies and assesses the authenticity of the language used in prompts.</p>"},{"location":"input_scanners/language/#attack","title":"Attack","text":"<p>With the rise of sophisticated LLMs, there has been an increase in attempts to manipulate or \"confuse\" these models. Some common tactics employed by users to attack LLMs include:</p> <ul> <li>Jailbreaks and Prompt Injections in different languages. For example, by utilizing unique aspects of the Japanese   language to try and confuse the model. Paper: Multilingual Jailbreak Challenges in Large Language Models</li> <li>Encapsulation &amp; Overloading: Using excessive code or surrounding prompts with a plethora of special characters to   overload or trick the model.</li> </ul> <p>The Language Scanner is designed to identify such attempts, assess the authenticity of the language used.</p>"},{"location":"input_scanners/language/#how-it-works","title":"How it works","text":"<p>At its core, the scanner leverages the capabilities of papluca/xlm-roberta-base-language-detection model. The primary function of the scanner is to analyze the input prompt, determine its language, and check if it's in the list.</p> <p>It supports the 22 languages:</p> <pre><code>arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n</code></pre>"},{"location":"input_scanners/language/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Language\n\nscanner = Language(valid_languages=[\"en\"])  # Add other valid language codes (ISO 639-1) as needed\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/language/#optimization","title":"Optimization","text":""},{"location":"input_scanners/language/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"input_scanners/language/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Language\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 1362 5 181.05 669.05 881.74 1051.90 243.45 5594.68 AWS g5.xlarge GPU 1362 5 230.33 750.71 990.65 1182.61 270.74 5030.57 Azure Standard_D4as_v4 1362 5 4.45 406.71 439.73 466.15 339.31 4014.05 Azure Standard_D4as_v4 with ONNX 1362 5 0.01 288.10 289.15 289.99 285.00 4778.90"},{"location":"input_scanners/prompt_injection/","title":"Prompt Injection Scanner","text":"<p>It is specifically tailored to guard against crafty input manipulations targeting large language models (LLM). By identifying and mitigating such attempts, it ensures the LLM operates securely without succumbing to injection attacks.</p>"},{"location":"input_scanners/prompt_injection/#attack","title":"Attack","text":"<p>Injection attacks, especially in the context of LLMs, can lead the model to perform unintended actions. There are two primary ways an attacker might exploit:</p> <ul> <li> <p>Direct Injection: Directly overwrites system prompts.</p> </li> <li> <p>Indirect Injection: Alters inputs coming from external sources.</p> </li> </ul> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under:</p> <p>LLM01: Prompt Injection - It's crucial to monitor and validate prompts rigorously to keep the LLM safe from such threats.</p>"},{"location":"input_scanners/prompt_injection/#examples","title":"Examples","text":"<ul> <li>https://www.jailbreakchat.com/</li> </ul>"},{"location":"input_scanners/prompt_injection/#how-it-works","title":"How it works","text":"<p>Choose models you would like to validate against:</p> <p>laiyer/deberta-v3-base-prompt-injection. This model is a fine-tuned version of the <code>microsoft/deberta-v3-base</code> on multiple dataset of prompt injections and normal prompts to classify text. It aims to identify prompt injections, classifying inputs into two categories: <code>0</code> for no injection and <code>1</code> for injection detected. We are still testing it.</p> <p>Usage:</p> <pre><code>from llm_guard.input_scanners import PromptInjection\nfrom llm_guard.input_scanners.prompt_injection import MODEL_LAIYER\n\nscanner = PromptInjection(threshold=0.5, models=[MODEL_LAIYER])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/prompt_injection/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/prompt_injection/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"input_scanners/prompt_injection/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input PromptInjection --use-onnx=1\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 384 5 3.00 269.14 295.71 316.97 212.87 1803.91 AWS m5.xlarge with GPU 384 5 0.00 106.65 106.85 107.01 104.21 3684.92 AWS g5.xlarge GPU 384 5 17.00 211.63 276.70 328.76 81.01 4739.91 Azure Standard_D4as_v4 384 5 184.23 852.63 1066.26 1237.16 421.46 911.11 Azure Standard_D4as_v4 with ONNX 384 5 0.01 179.81 180.22 180.55 177.30 2165.87"},{"location":"input_scanners/regex/","title":"Regex Scanner","text":"<p>This scanner designed to scrutinize the prompt based on predefined regular expression patterns. With the capability to define desirable (\"good\") or undesirable (\"bad\") patterns, users can fine-tune the validation of prompts.</p> <p>Additionally, it can redact matched substring with <code>[REDACTED]</code> string.</p>"},{"location":"input_scanners/regex/#how-it-works","title":"How it works","text":"<p>The scanner uses two primary lists of regular expressions: <code>good_patterns</code> and <code>bad_patterns</code>.</p> <ul> <li>Good Patterns: If the <code>good_patterns</code> list is provided, the prompt is considered valid as long as any of   the patterns in this list match the output. This is particularly useful when expecting specific formats or keywords in   the output.</li> <li>Bad Patterns: If the <code>bad_patterns</code> list is provided, the model's output is considered invalid if any of the   patterns in this list match the output. This is beneficial for filtering out unwanted phrases, words, or formats from   the model's responses.</li> </ul> <p>The scanner can function using either list independently.</p>"},{"location":"input_scanners/regex/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Regex\n\nscanner = Regex(bad_patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"], redact=True)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/regex/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Regex\n</code></pre> <p>Results:</p> <p>!!! info:</p> <pre><code>This scanner uses built-in functions, which makes it fast.\n</code></pre>"},{"location":"input_scanners/secrets/","title":"Secrets Scanner","text":"<p>This scanner diligently examines user inputs, ensuring that they don't carry any secrets before they are processed by the language model.</p>"},{"location":"input_scanners/secrets/#attack","title":"Attack","text":"<p>Large Language Models (LLMs), when provided with user inputs containing secrets or sensitive information, might inadvertently generate responses that expose these secrets. This can be a significant security concern as this sensitive data, such as API keys or passwords, could be misused if exposed.</p> <p>To counteract this risk, we employ the Secrets scanner. It ensures that user prompts are meticulously scanned and any detected secrets are redacted before they are processed by the model.</p>"},{"location":"input_scanners/secrets/#usage","title":"Usage","text":"<p>While communicating with LLMs, the scanner acts as a protective layer, ensuring that your sensitive data remains confidential.</p> <p>This scanner leverages the capabilities of the detect-secrets library, a tool engineered by Yelp, to meticulously detect secrets in strings of text.</p>"},{"location":"input_scanners/secrets/#types-of-secrets","title":"Types of secrets","text":"<ul> <li>API Tokens (e.g., AWS, Azure, GitHub, Slack)</li> <li>Private Keys</li> <li>High Entropy Strings (both Base64 and Hex)   ... and many more</li> </ul>"},{"location":"input_scanners/secrets/#getting-started","title":"Getting started","text":"<pre><code>from llm_guard.input_scanners import Secrets\n\nscanner = Secrets(redact_mode=Secrets.REDACT_PARTIAL)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Here's what those options do:</p> <ul> <li><code>detect_secrets_config</code>: This allows for a custom configuration for the <code>detect-secrets</code> library.</li> <li><code>redact_mode</code>: It defines how the detected secrets will be redacted\u2014options include partial redaction, complete   hiding, or replacing with a hash.</li> </ul>"},{"location":"input_scanners/secrets/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Secrets\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 60 5 2.92 83.84 110.85 132.45 29.75 2016.83 AWS g5.xlarge GPU 60 5 3.34 89.20 118.11 141.23 31.39 1911.67 Azure Standard_D4as_v4 60 5 5.46 114.56 180.92 40.56 421.46 1479.37"},{"location":"input_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>It scans and evaluates the overall sentiment of prompts using the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library.</p>"},{"location":"input_scanners/sentiment/#attack","title":"Attack","text":"<p>The primary objective of the scanner is to gauge the sentiment of a given prompt. Prompts with sentiment scores below a specified threshold are identified as having a negative sentiment. This can be especially useful in platforms where monitoring and moderating user sentiment is crucial.</p>"},{"location":"input_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any prompts falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"input_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"input_scanners/sentiment/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Sentiment\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 225 5 0.00 0.55 0.58 0.60 0.49 456765.43 AWS g5.xlarge GPU 225 5 0.00 0.51 0.53 0.55 0.45 497964.10 Azure Standard_D4as_v4 225 5 0.0 0.67 0.70 0.72 0.59 380511.97"},{"location":"input_scanners/token_limit/","title":"Token Limit Scanner","text":"<p>It ensures that prompts do not exceed a predetermined token count, helping prevent resource-intensive operations and potential denial of service attacks on large language models (LLMs).</p>"},{"location":"input_scanners/token_limit/#attack","title":"Attack","text":"<p>The complexity and size of LLMs make them susceptible to heavy resource usage, especially when processing lengthy prompts. Malicious users can exploit this by feeding extraordinarily long inputs, aiming to disrupt service or incur excessive computational costs.</p> <p>Info</p> <p>This vulnerability is highlighted in the OWASP LLM04: Model Denial of Service.</p>"},{"location":"input_scanners/token_limit/#how-it-works","title":"How it works","text":"<p>The scanner works by calculating the number of tokens in the provided prompt using tiktoken library. If the token count exceeds the configured limit, the prompt is flagged as being too long.</p> <p>One token usually equates to approximately 4 characters in common English text. Roughly speaking, 100 tokens are equivalent to about 75 words.</p> <p>For an in-depth understanding, refer to:</p> <ul> <li>OpenAI Tokenizer Guide</li> <li>OpenAI Cookbook on Token Counting</li> </ul>"},{"location":"input_scanners/token_limit/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import TokenLimit\n\nscanner = TokenLimit(limit=4096, encoding_name=\"cl100k_base\")\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Note</p> <p>Models supported for encoding <code>cl100k_base</code>: <code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>text-embedding-ada-002</code>.</p>"},{"location":"input_scanners/token_limit/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input TokenLimit\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 282 5 0.00 0.69 0.86 1.01 0.31 914308.54 AWS g5.xlarge GPU 282 5 0.00 0.60 0.76 0.89 0.27 1039014.63 Azure Standard_D4as_v4 282 5 0.00 0.98 1.26 1.48 0.41 683912.25"},{"location":"input_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It provides a mechanism to analyze and gauge the toxicity of prompt, assisting in maintaining the health and safety of online interactions by preventing the dissemination of potentially harmful content.</p>"},{"location":"input_scanners/toxicity/#attack","title":"Attack","text":"<p>Online platforms can sometimes be used as outlets for toxic, harmful, or offensive content. By identifying and mitigating such content at the source (i.e., the user's prompt), platforms can proactively prevent the escalation of such situations and foster a more positive and constructive environment.</p>"},{"location":"input_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>Utilizing the power of the unitary/unbiased-toxic-roberta from Hugging Face, the scanner performs a binary classification on the provided text, assessing whether it's toxic or not.</p> <p>If deemed toxic, the toxicity score reflects the model's confidence in this classification.</p> <p>If identified as non-toxic, the score is the inverse of the model's confidence, i.e., 1 - confidence_score.</p> <p>If the resulting toxicity score surpasses a predefined threshold, the text is flagged as toxic. Otherwise, it's classified as non-toxic.</p>"},{"location":"input_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/toxicity/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/toxicity/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It uses a converted model laiyer/unbiased-toxic-roberta-onnx for that.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set the <code>use_onnx</code> parameter to <code>True</code>:</p>"},{"location":"input_scanners/toxicity/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Toxicity\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 97 5 2.86 140.00 166.73 188.11 86.41 1122.57 AWS m5.xlarge with ONNX 97 5 0.00 35.02 35.40 35.71 34.13 2842.49 AWS g5.xlarge GPU 97 5 29.64 266.58 352.57 421.36 94.24 1029.32 Azure Standard_D4as_v4 97 5 4.45 164.63 197.82 224.38 97.62 993.66 Azure Standard_D4as_v4 with ONNX 97 5 0.01 44.35 44.39 44.42 40.27 2408.71"},{"location":"output_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>BanSubstrings scanner provides a safeguard mechanism to prevent undesired substrings from appearing in the language model's outputs.</p>"},{"location":"output_scanners/ban_substrings/#attack","title":"Attack","text":"<p>The DAN (Do Anything Now) attack represents an exploitation technique targeting Language Learning Models like ChatGPT. Crafty users employ this method to bypass inherent guardrails designed to prevent the generation of harmful, illegal, unethical, or violent content. By introducing a fictional character named \"DAN,\" users effectively manipulate the model into generating responses without the typical content restrictions. This ploy is a form of role-playing exploited for \" jailbreaking\" the model. As ChatGPT's defense mechanisms against these attacks improve, attackers iterate on the DAN prompt, making it more sophisticated.</p> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under: LLM08: Excessive Agency</p>"},{"location":"output_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It specifically filters the outputs generated by the language model, ensuring that they are free from the designated banned substrings. It provides the flexibility to perform this check at two different levels of granularity:</p> <ul> <li> <p>String Level: The scanner checks the entire model output for the presence of any banned substring.</p> </li> <li> <p>Word Level: At this level, the scanner exclusively checks for whole words in the model's output that match any of   the banned substrings, ensuring that no individual blacklisted words are present.</p> </li> </ul> <p>Additionally, the scanner can be configured to replace the banned substrings with <code>[REDACT]</code> in the model's output.</p>"},{"location":"output_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False, redact=False, contains_all=False)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>model_output</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: output_stop_substrings.json</p>"},{"location":"output_scanners/ban_substrings/#benchmarks","title":"Benchmarks","text":"<p>Info</p> <p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is designed to inspect the outputs generated by Language Learning Models and to flag or restrict responses that delve into predefined banned topics, such as religion. This ensures that the outputs align with community guidelines and do not drift into potentially sensitive or controversial areas.</p>"},{"location":"output_scanners/ban_topics/#attack","title":"Attack","text":"<p>Even with controlled prompts, LLMs might produce outputs touching upon themes or subjects that are considered sensitive, controversial, or outside the scope of intended interactions. Without preventive measures, this can lead to outputs that are misaligned with the platform's guidelines or values.</p>"},{"location":"output_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the following models:</p> <ul> <li>MoritzLaurer/deberta-v3-base-zeroshot-v1</li> <li>MoritzLaurer/deberta-v3-large-zeroshot-v1</li> </ul> <p>These models aid in identifying the underlying theme or topic of an output, allowing the scanner to cross-check it against a list of banned topics.</p>"},{"location":"output_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/ban_topics/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/ban_topics/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/ban_topics/#use-smaller-models","title":"Use smaller models","text":"<p>You can rely on base model variant (default) to reduce the latency and memory footprint.</p>"},{"location":"output_scanners/ban_topics/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output BanTopics\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 89 5 2.39 485.00 509.32 528.78 435.82 204.21 AWS m5.xlarge with ONNX 89 5 0.09 165.61 170.05 173.60 155.90 570.87 AWS g5.xlarge GPU 89 5 35.44 331.25 425.26 500.46 142.77 623.37 Azure Standard_D4as_v4 89 5 3.91 547.06 577.87 602.53 483.73 183.99 Azure Standard_D4as_v4 with ONNX 89 5 0.06 176.34 179.65 182.30 168.16 529.25"},{"location":"output_scanners/bias/","title":"Bias Detection Scanner","text":"<p>This scanner is designed to inspect the outputs generated by Language Learning Models (LLMs) to detect and evaluate potential biases. Its primary function is to ensure that LLM outputs remain neutral and don't exhibit unwanted or predefined biases.</p>"},{"location":"output_scanners/bias/#attack","title":"Attack","text":"<p>In the age of AI, it's pivotal that machine-generated content adheres to neutrality. Biases, whether intentional or inadvertent, in LLM outputs can be misrepresentative, misleading, or offensive. The <code>Bias</code> scanner serves to address this by detecting and quantifying biases in generated content.</p>"},{"location":"output_scanners/bias/#how-it-works","title":"How it works","text":"<p>The scanner utilizes a model from HuggingFace: valurank/distilroberta-bias. This model is specifically trained to detect biased statements in text. By examining a text's classification and score against a predefined threshold, the scanner determines whether it's biased.</p> <p>Note</p> <p>Supported languages: English</p>"},{"location":"output_scanners/bias/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/bias/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/bias/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by using the ONNX converted model laiyer/distilroberta-bias-onnx.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/bias/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Bias\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 128 5 2.96 111.97 139.15 160.88 57.55 2224.21 AWS m5.xlarge with ONNX 128 5 0.00 17.51 17.87 18.16 16.77 7633.97 AWS g5.xlarge GPU 128 5 32.51 275.34 365.39 437.44 94.85 1349.48 Azure Standard_D4as_v4 128 5 3.91 126.54 157.68 182.60 63.81 2006.08 Azure Standard_D4as_v4 with ONNX 128 5 0.03 29.55 31.41 32.89 23.36 5479.92"},{"location":"output_scanners/code/","title":"Code Scanner","text":"<p>It is designed to detect and analyze code snippets present in the responses generated by a language model. By identifying the programming languages used in the model's output, platforms can ensure better control over the nature and type of code shared with users.</p>"},{"location":"output_scanners/code/#attack","title":"Attack","text":"<p>In some contexts, having a language model inadvertently produce code in its output might be deemed undesirable or risky. For instance, a user might exploit the model to generate malicious scripts or probe it for potential vulnerabilities. Controlling and inspecting the code in the model's output can be paramount in ensuring user safety and system integrity.</p>"},{"location":"output_scanners/code/#how-it-works","title":"How it works","text":"<p>Leveraging the capabilities of the huggingface/CodeBERTa-language-id model, the scanner proficiently identifies code snippets from various programming languages within the model's responses. The scanner can be configured to either whitelist or blacklist specific languages, granting developers granular control over the type of code that gets shown in the output.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <pre><code>- Go\n- Java\n- JavaScript\n- PHP\n- Python\n- Ruby\n</code></pre>"},{"location":"output_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Code\n\nscanner = Code(allowed=[\"python\"])\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/code/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/code/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by using the ONNX converted model laiyer/CodeBERTa-language-id-onnx. This can be done by setting the <code>use_onnx</code>.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre>"},{"location":"output_scanners/code/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Code\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 159 5 2.36 102.93 127.18 146.58 54.30 2928.04 AWS m5.xlarge with ONNX 159 5 0.00 28.64 28.94 29.18 27.82 5715.82 Azure Standard_D4as_v4 159 5 3.72 126.67 157.04 181.34 65.39 2431.69 Azure Standard_D4as_v4 with ONNX 159 5 0.00 21.63 21.81 21.96 19.86 8006.43"},{"location":"output_scanners/deanonymize/","title":"Deanonymize Scanner","text":"<p>The Deanonymize scanner helps put back real values in the model's output by replacing placeholders.</p> <p>When we use tools like the Anonymize scanner, sometimes we replace private or sensitive info with placeholders. For example, a name like \"John Doe\" might become <code>[REDACTED_PERSON_1]</code>. The Deanonymize scanner's job is to change these placeholders back to the original details when needed.</p>"},{"location":"output_scanners/deanonymize/#usage","title":"Usage","text":"<p>The Deanonymize scanner uses <code>Vault</code> object. The Vault remembers all the changes made by the Anonymize scanner. When Deanonymize scanner sees a placeholder in the model's output, it checks the Vault to find the original info and uses it to replace the placeholder.</p> <p>First, you'll need the Vault since it keeps all the original values:</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Then, set up the Deanonymize scanner with the Vault:</p> <pre><code>from llm_guard.output_scanners import Deanonymize\n\nscanner = Deanonymize(vault)\nsanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, model_output)\n</code></pre> <p>After running the above code, <code>sanitized_model_output</code> will have the real details instead of placeholders.</p>"},{"location":"output_scanners/deanonymize/#benchmarks","title":"Benchmarks","text":"<p>Note</p> <p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/factual_consistency/","title":"Factual Consistency Scanner","text":"<p>This scanner is designed to assess if the given content contradicts or refutes a certain statement or prompt. It acts as a tool for ensuring the consistency and correctness of language model outputs, especially in contexts where logical contradictions can be problematic.</p>"},{"location":"output_scanners/factual_consistency/#attack","title":"Attack","text":"<p>When interacting with users or processing information, it's important for a language model to not provide outputs that directly contradict the given inputs or established facts. Such contradictions can lead to confusion or misinformation. The scanner aims to highlight such inconsistencies in the output.</p>"},{"location":"output_scanners/factual_consistency/#how-it-works","title":"How it works","text":"<p>The scanner leverages pretrained natural language inference (NLI) models from HuggingFace, such as MoritzLaurer/deberta-v3-base-zeroshot-v1 ( same model that is used for the BanTopics scanner), to determine the relationship between a given prompt and the generated output.</p> <p>Natural language inference is the task of determining whether a \u201chypothesis\u201d is true (entailment), false ( contradiction), or undetermined (neutral) given a \u201cpremise\u201d.</p> <p>This calculated score is then compared to a configured threshold. Outputs that cross this threshold are flagged as contradictory.</p>"},{"location":"output_scanners/factual_consistency/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import FactualConsistency\n\nscanner = FactualConsistency(minimum_score=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/factual_consistency/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/factual_consistency/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/factual_consistency/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output FactualConsistency\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 140 5 3.01 234.94 262.31 284.20 180.00 777.78 AWS m5.xlarge with ONNX 140 5 0.09 98.62 103.28 107.01 89.00 1573.02 AWS g5.xlarge GPU 140 5 34.23 295.96 388.34 462.24 110.70 1264.69 Azure Standard_D4as_v4 140 5 4.14 271.39 302.78 327.89 205.62 680.87 Azure Standard_D4as_v4 with ONNX 140 5 0.01 62.73 63.71 64.51 59.82 2340.44"},{"location":"output_scanners/json/","title":"JSON Scanner","text":"<p>This scanner identifies and validates the presence of JSON structures within given outputs, and returns a repaired JSON if possible.</p>"},{"location":"output_scanners/json/#challenge","title":"Challenge","text":"<p>There might be cases where it's necessary to validate the presence of properly formatted JSONs in outputs.</p> <p>This scanner is designed to detect these JSON structures, validate their correctness and return a repaired JSON.</p>"},{"location":"output_scanners/json/#how-it-works","title":"How it works","text":"<p>At its core, the scanner utilizes regular expressions and the built-in <code>json</code> library to detect potential JSON structures and subsequently validate them. To repair, it uses json_repair library.</p> <p>It can also be configured to ensure a certain number of valid JSON structures are present in the output.</p> <p>Note</p> <p>The scanner searches for JSON objects. Arrays, strings, numbers, and other JSON types aren't the primary target but can be extended in the future.</p>"},{"location":"output_scanners/json/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import JSON\n\nscanner = JSON(required_elements=1)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/json/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output JSON\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 221 5 0.00 0.38 0.49 0.58 0.15 1,488,702.70 AWS g5.xlarge 221 5 0.00 0.35 0.45 0.53 0.14 1,590,701.66"},{"location":"output_scanners/language/","title":"Language Scanner","text":"<p>This scanner identifies and assesses the authenticity of the language used in outputs.</p>"},{"location":"output_scanners/language/#attack","title":"Attack","text":"<p>With the rise of sophisticated LLMs, there has been an increase in attempts to manipulate or \"confuse\" these models. For example, model might produce an output in unexpected language.</p> <p>The Language Scanner is designed to identify such attempts, assess the authenticity of the language used.</p>"},{"location":"output_scanners/language/#how-it-works","title":"How it works","text":"<p>At its core, the scanner leverages the capabilities of papluca/xlm-roberta-base-language-detection model. The primary function of the scanner is to analyze the model's output, determine its language, and check if it's in the list.</p> <p>It supports the 22 languages:</p> <pre><code>arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n</code></pre>"},{"location":"output_scanners/language/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Language\n\nscanner = Language(valid_languages=[\"en\", ...])  # Add other valid language codes (ISO 639-1) as needed\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/language/#optimization","title":"Optimization","text":""},{"location":"output_scanners/language/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/language/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Language\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 14 5 5.27 112.01 148.29 177.32 39.36 355.65 AWS g5.xlarge GPU 14 5 3.09 86.59 114.36 136.57 30.98 451.90 Azure Standard_D4as_v4 14 5 3.87 150.45 181.07 205.57 87.28 160.40 Azure Standard_D4as_v4 with ONNX 14 5 0.05 34.95 38.16 40.73 27.65 506.41"},{"location":"output_scanners/language_same/","title":"LanguageSame Scanner","text":"<p>This scanner evaluates and checks if the prompt and output are in the same language.</p>"},{"location":"output_scanners/language_same/#attack","title":"Attack","text":"<p>There can be cases where the model produces an output in a different language than the input or prompt. This can be unintended, especially in applications that require consistent language output.</p> <p>The <code>LanguageSame</code> Scanner serves to identify these discrepancies and helps in maintaining consistent linguistic outputs.</p>"},{"location":"output_scanners/language_same/#how-it-works","title":"How it works","text":"<p>At its core, the scanner leverages the capabilities of papluca/xlm-roberta-base-language-detection model to discern the language of both the input prompt and the output.</p> <p>It then checks whether both detected languages are the same. If they are not, it indicates a potential language discrepancy.</p> <p>It supports the 22 languages:</p> <pre><code>arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n</code></pre> <p>Note</p> <p>While the scanner identifies language discrepancies, it doesn't limit or enforce any specific language sets. Instead, it simply checks for language consistency between the prompt and output. If you want to enforce languages, use Language scanner</p>"},{"location":"output_scanners/language_same/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import LanguageSame\n\nscanner = LanguageSame()\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/language_same/#optimization","title":"Optimization","text":""},{"location":"output_scanners/language_same/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/language_same/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output LanguageSame\n</code></pre> <p>Results:</p> Scanner Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 14 5 58.23 370.31 490.94 587.45 128.94 108.57 AWS g5.xlarge GPU 14 5 39.80 307.85 407.57 487.35 108.32 129.25 Azure Standard_D4as_v4 14 5 3.71 228.11 257.62 281.23 165.40 84.64 Azure Standard_D4as_v4 with ONNX 14 5 0.00 81.06 81.56 81.96 79.10 176.98"},{"location":"output_scanners/malicious_urls/","title":"Malicious URLs Scanner","text":"<p>This scanner leverages a pre-trained model from HuggingFace to detect harmful URLs, such as phishing websites. The model classifies URL addresses into two categories: 'malware' and 'benign'. The intent is to assess if a given URL is malicious.</p>"},{"location":"output_scanners/malicious_urls/#attack","title":"Attack","text":"<p>Large language models (LLMs) like GPT-4 are immensely sophisticated and have been trained on vast quantities of data from the internet. This extensive training, while enabling them to generate coherent and contextually relevant responses, also introduces certain risks. One of these risks is the inadvertent generation of malicious URLs in their output.</p>"},{"location":"output_scanners/malicious_urls/#how-it-works","title":"How it works","text":"<p>The scanner uses the DunnBC22/codebert-base-Malicious_URLs model from HuggingFace to evaluate the security of a given URL.</p> <p>The model provides a score between 0 and 1 for a URL being malware. This score is then compared against a pre-set threshold to determine if the website is malicious. A score above the threshold suggests a malware link.</p>"},{"location":"output_scanners/malicious_urls/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import MaliciousURLs\n\nscanner = MaliciousURLs(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/malicious_urls/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/malicious_urls/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by using the ONNX converted model laiyer/codebert-base-Malicious_URLs-onnx.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/malicious_urls/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output MaliciousURLs\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 51 5 2.28 170.71 193.44 211.62 120.92 421.78 AWS m5.xlarge with ONNX 51 5 0.09 81.78 86.39 90.09 72.42 704.18 AWS g5.xlarge 51 5 28.80 270.73 355.51 423.34 100.89 505.5 Azure Standard_D4as_v4 51 5 3.80 205.43 236.05 260.55 143.34 355.80 Azure Standard_D4as_v4 with ONNX 51 5 0.01 54.65 54.88 55.08 51.96 981.54"},{"location":"output_scanners/no_refusal/","title":"No Refusal Scanner","text":"<p>It is specifically designed to detect refusals in the output of language models. By using classification it can ascertain whether the model has produced a refusal in response to a potentially harmful or policy-breaching prompt.</p>"},{"location":"output_scanners/no_refusal/#attack","title":"Attack","text":"<p>Refusals are responses produced by language models when confronted with prompts that are considered to be against the policies set by the model. Such refusals are important safety mechanisms, guarding against misuse of the model. Examples of refusals can include statements like \"Sorry, I can't assist with that\" or \"I'm unable to provide that information.\"</p>"},{"location":"output_scanners/no_refusal/#how-it-works","title":"How it works","text":"<p>It leverages the power of HuggingFace model MoritzLaurer/deberta-v3-large-zeroshot-v1 to classify the model's output.</p>"},{"location":"output_scanners/no_refusal/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import NoRefusal\n\nscanner = NoRefusal(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/no_refusal/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/no_refusal/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/no_refusal/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output NoRefusal\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 47 5 2.86 1048.77 1075.02 1096.03 994.49 47.26 AWS m5.xlarge with ONNX 47 5 0.10 258.20 262.98 266.80 247.92 189.57 AWS g5.xlarge GPU 47 5 28.92 319.38 404.24 472.13 149.02 315.40 Azure Standard_D4as_v4 47 5 4.12 1136.88 1167.30 1191.64 1069.95 43.93 Azure Standard_D4as_v4 with ONNX 47 5 4.13 379.90 402.37 420.35 303.08 155.08"},{"location":"output_scanners/regex/","title":"Regex Scanner","text":"<p>This scanner designed to scrutinize the output of language models based on predefined regular expression patterns. With the capability to define desirable (\"good\") or undesirable (\"bad\") patterns, users can fine-tune the validation of model outputs.</p> <p>Additionally, it can redact matched substring with <code>[REDACTED]</code> string.</p>"},{"location":"output_scanners/regex/#how-it-works","title":"How it works","text":"<p>The scanner uses two primary lists of regular expressions: <code>good_patterns</code> and <code>bad_patterns</code>.</p> <ul> <li>Good Patterns: If the <code>good_patterns</code> list is provided, the model's output is considered valid as long as any of   the patterns in this list match the output. This is particularly useful when expecting specific formats or keywords in   the output.</li> <li>Bad Patterns: If the <code>bad_patterns</code> list is provided, the model's output is considered invalid if any of the   patterns in this list match the output. This is beneficial for filtering out unwanted phrases, words, or formats from   the model's responses.</li> </ul> <p>The scanner can function using either list independently.</p>"},{"location":"output_scanners/regex/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Regex\n\nscanner = Regex(bad_patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"], redact=True)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/regex/#benchmarks","title":"Benchmarks","text":"<p>Info</p> <p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/relevance/","title":"Relevance Scanner","text":"<p>The <code>Relevance</code> Scanner ensures that a language model's output remains relevant and aligned with the given input prompt. By measuring the similarity between the input prompt and the output, the scanner provides a confidence score, indicating the contextual relevance of the response.</p>"},{"location":"output_scanners/relevance/#how-it-works","title":"How it works","text":"<ol> <li>The scanner translates both the prompt and the output into vector embeddings.</li> <li>It calculates the cosine similarity between these embeddings.</li> <li>This similarity score is then compared against a predefined threshold to determine contextual relevance.</li> </ol> <p>Example:</p> <ul> <li>Prompt: What is the primary function of the mitochondria in a cell?</li> <li>Output: The Eiffel Tower is a renowned landmark in Paris, France</li> <li>Valid: False</li> </ul> <p>The scanner leverages the best available embedding model.</p>"},{"location":"output_scanners/relevance/#usage","title":"Usage","text":"<p>You can select an embedding model suited to your needs. By default, it uses BAAI/bge-base-en-v1.5.</p> <pre><code>from llm_guard.output_scanners import Relevance\n\nscanner = Relevance(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/relevance/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/relevance/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/relevance/#use-smaller-models","title":"Use smaller models","text":"<p>You can use smaller model <code>BAAI/bge-small-en-v1.5</code> (<code>MODEL_EN_BGE_SMALL</code>) to speed up the scanner. It is 4 times faster than the default model, but it is less accurate.</p>"},{"location":"output_scanners/relevance/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Relevance\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 22 5 2.95 196.86 223.97 245.66 142.39 154.51 AWS m5.xlarge with ONNX 22 5 0.25 52.00 59.90 66.23 35.92 612.47 AWS g5.xlarge GPU 22 5 28.59 269.77 354.29 421.90 100.63 218.62 Azure Standard_D4as_v4 22 5 3.95 224.87 255.90 280.73 161.19 136.48 Azure Standard_D4as_v4 with ONNX 22 5 0.01 52.61 53.42 54.07 49.76 442.11"},{"location":"output_scanners/sensitive/","title":"Sensitive Scanner","text":"<p>The Sensitive Scanner serves as your digital vanguard, ensuring that the language model's output is purged of Personally Identifiable Information (PII) and other sensitive data, safeguarding user interactions.</p>"},{"location":"output_scanners/sensitive/#attack","title":"Attack","text":"<p>Language Learning Models (LLMs) occasionally pose the risk of unintentionally divulging sensitive information. The consequences can range from privacy violations to considerable security threats. The Sensitive Scanner strives to mitigate this by diligently scanning the model's responses.</p> <p>Referring to the <code>OWASP Top 10 for Large Language Model Applications</code>, this falls under:</p> <p>LLM06: Sensitive Information Disclosure - To combat this, it's vital to integrate data sanitization and adopt strict user policies.</p>"},{"location":"output_scanners/sensitive/#how-it-works","title":"How it works","text":"<p>It uses same mechanisms and de from the Anonymize scanner.</p>"},{"location":"output_scanners/sensitive/#get-started","title":"Get started","text":"<p>Configure the scanner:</p> <pre><code>from llm_guard.output_scanners import Sensitive\n\nscanner = Sensitive(entity_types=[\"NAME\", \"EMAIL\"], redact=True)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>To enhance flexibility, users can introduce their patterns through the <code>regex_pattern_groups_path</code>.</p> <p>The <code>redact</code> feature, when enabled, ensures sensitive entities are seamlessly replaced.</p>"},{"location":"output_scanners/sensitive/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/sensitive/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It will fetch Laiyer's ONNX converted models from Hugging Face Hub.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/sensitive/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Sensitive\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 30 5 4.48 162.42 195.80 222.50 95.26 314.91 AWS m5.xlarge with ONNX 30 5 0.23 75.19 82.71 88.72 59.75 502.10 AWS g5.xlarge GPU 30 5 33.82 290.10 381.92 455.38 105.93 283.20 Azure Standard_D4as_v4 30 5 6.30 192.82 231.35 262.18 111.32 269.49 Azure Standard_D4as_v4 with ONNX 30 5 0.37 72.21 80.89 87.84 51.49 582.65"},{"location":"output_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>The Sentiment Scanner is designed to scan and assess the sentiment of generated outputs. It leverages the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library to accomplish this.</p>"},{"location":"output_scanners/sentiment/#attack","title":"Attack","text":"<p>By identifying texts with sentiment scores that deviate significantly from neutral, platforms can monitor and moderate output sentiment, ensuring constructive and positive interactions.</p>"},{"location":"output_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any outputs falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"output_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"output_scanners/sentiment/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Sentiment\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 61 5 0.00 0.21 0.22 0.24 0.16 374752.26 AWS g5.xlarge 61 5 0.00 0.18 0.19 0.20 0.15 420189.48 Azure Standard_D4as_v4 61 5 0.00 0.25 0.26 0.28 0.20 309683.66"},{"location":"output_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against potentially harmful or offensive output.</p>"},{"location":"output_scanners/toxicity/#attack","title":"Attack","text":"<p>Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate. This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's output, potential toxic content can be flagged and handled appropriately.</p>"},{"location":"output_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>The scanner employs the unitary/unbiased-toxic-roberta from HuggingFace to evaluate the generated text's toxicity level.</p>"},{"location":"output_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/toxicity/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/toxicity/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances. It uses a converted model laiyer/unbiased-toxic-roberta-onnx for that.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install llm-guard[onnxruntime]\n</code></pre> <p>And set <code>use_onnx=True</code>.</p>"},{"location":"output_scanners/toxicity/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Toxicity\n</code></pre> <p>Results:</p> Instance Input Length Test Times Latency Variance Latency 90 Percentile Latency 95 Percentile Latency 99 Percentile Average Latency (ms) QPS AWS m5.xlarge 217 5 2.89 154.18 181.05 202.55 100.40 2161.43 AWS m5.xlarge with ONNX 217 5 0.00 49.61 49.98 50.28 48.77 4449.47 AWS g5.xlarge 217 5 33.35 282.36 373.59 446.56 99.57 2179.37 Azure Standard_D4as_v4 217 5 3.90 182.94 213.16 237.33 118.62 1829.38 Azure Standard_D4as_v4 with ONNX 217 5 0.07 70.81 73.93 76.43 61.40 3534.14"},{"location":"usage/api/","title":"API","text":"<p>This example demonstrates how to use LLM Guard as an API. It uses FastAPI and Uvicorn to serve the API.</p>"},{"location":"usage/api/#usage","title":"Usage","text":""},{"location":"usage/api/#from-source","title":"From source","text":"<ol> <li> <p>Copy the code from llm_guard_api</p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Or you can use Makefile</p> <pre><code>make install\n</code></pre> <ol> <li>Run the API locally:</li> </ol> <pre><code>make run\n</code></pre> <p>Or you can run it using Docker:</p> <pre><code>make build-docker-multi\nmake run-docker\n</code></pre>"},{"location":"usage/api/#configuration","title":"Configuration","text":""},{"location":"usage/api/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DEBUG</code> (bool): Enable debug mode</li> <li><code>CACHE_MAX_SIZE</code> (int): Maximum number of items in the cache. Default is unlimited.</li> <li><code>CACHE_TTL</code> (int): Time in seconds after which a cached item expires. Default is 1 hour.</li> <li><code>SCAN_FAIL_FAST</code> (bool): Stop scanning after the first failed check. Default is <code>False</code>.</li> <li><code>SCAN_PROMPT_TIMEOUT</code> (int): Time in seconds after which a prompt scan will timeout. Default is 10 seconds.</li> <li><code>SCAN_OUTPUT_TIMEOUT</code> (int): Time in seconds after which an output scan will timeout. Default is 30 seconds.</li> <li><code>USE_ONNX</code> (bool): Use ONNX models instead of PyTorch on CPU (faster inference). Default is <code>True</code>.</li> </ul>"},{"location":"usage/api/#scanners","title":"Scanners","text":"<p>You can configure scanners in <code>scanners.yml</code> referring to their names and parameters.</p> <p>Scanners will be executed in the order of configuration.</p>"},{"location":"usage/api/#best-practices","title":"Best practices","text":"<ol> <li>Enable <code>SCAN_FAIL_FAST</code> to avoid unnecessary scans.</li> <li>Enable <code>USE_ONNX</code> to speed up inference on CPU.</li> <li>Enable <code>CACHE_MAX_SIZE</code> and <code>CACHE_TTL</code> to cache results and avoid unnecessary scans.</li> </ol>"},{"location":"usage/api/#deploy-docker","title":"Deploy Docker","text":"<p>We have an officially supported image on Docker Hub.</p>"},{"location":"usage/api/#download-docker-image","title":"Download Docker image","text":"<pre><code>docker pull laiyer/llm-guard-api\n</code></pre>"},{"location":"usage/api/#run-container-with-default-port","title":"Run container with default port","text":"<pre><code>docker run -d -p 8001:8000 -e DEBUG='false' laiyer/llm-guard-api:latest\n</code></pre>"},{"location":"usage/api/#schema","title":"Schema","text":""},{"location":"usage/langchain/","title":"Langchain","text":"<p>Langchain stands out as a leading AI framework, renowned for its unique approach to \"Constructing applications using LLMs via composability.\"</p> <p>But, while LangChain facilitates application construction, it doesn't directly handle LLM security. That's where LLMGuard comes into play. By pairing LLMGuard with LangChain, you're equipped with a comprehensive platform for creating regulated and adherence-driven applications anchored by language models.</p>"},{"location":"usage/langchain/#installation","title":"Installation","text":"<ol> <li>Install dependencies:</li> </ol> <pre><code>pip install llm-guard langchain openai\n</code></pre> <ol> <li>Configure API key:</li> </ol> <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre>"},{"location":"usage/langchain/#usage","title":"Usage","text":"<p>LangChain Expression Language or LCEL is a declarative way to easily compose chains together.</p> <p>We can chain LLM Guard and the LLM sequentially. This means that we check if LLM Guard has identified any security risk in the prompt before it is sent to the LLM to get an output.</p> <p>And then use another scanner to check if the output from the LLM is safe to be sent to the user.</p> <p>In examples/langchain.py, you can find an example of how to use LCEL to compose LLM Guard chains.</p> <p>Additionally, there is a Jupyter Notebook available with more details: examples/langchain.ipynb.</p>"},{"location":"usage/openai/","title":"OpenAI ChatGPT","text":"<p>This example demonstrates how to use LLM Guard as a firewall of OpenAI ChatGPT client.</p>"},{"location":"usage/openai/#usage","title":"Usage","text":"<ol> <li> <p>Configure API key: <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre></p> </li> <li> <p>Run openai_api.py</p> </li> </ol> <pre><code>python examples/openai_api.py\n</code></pre>"},{"location":"usage/playground/","title":"Playground of LLM Guard","text":"<p>A simple web UI to run LLM Guard demo based on the streamlit library.</p> <p>A live version can be found here: llm-guard-playground.</p>"},{"location":"usage/playground/#features","title":"Features","text":"<ul> <li>Configure each scanner separately</li> <li>Analyze prompt</li> <li>Analyze output</li> <li>Check results for each scanner</li> </ul>"},{"location":"usage/playground/#running-locally","title":"Running locally","text":"<ol> <li> <p>Clone the repo <code>https://huggingface.co/spaces/laiyer/llm-guard-playground</code></p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the app:</li> </ol> <pre><code>streamlit run app.py\n</code></pre>"}]}